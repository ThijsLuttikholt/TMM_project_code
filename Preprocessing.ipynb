{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text and Multimedia mining: Project preprocessing file\n",
    "\n",
    "This file contains the preprocessing code for the Text and multimedia mining project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All necessary imports for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "\n",
    "We will start by preprocessing the data. This is done to make any manipulations at a later point easier to manage.\n",
    "The cell below reads in the original data and removes the columns that will not be used at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('goodreads_reviews_spoiler.json.gz', lines=True, compression='gzip')\n",
    "data = data.drop(['review_id'],axis=1)\n",
    "print('finished preprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will divide the original data into 3 different datasets, one for each research question. This is done such that we can work on individual research questions rather than on the entire data. And this way, we don't have to do unnecessary computations if we only work with one of the datasets. We don't save the data to new files as writing to files is a very inefficient process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing\n"
     ]
    }
   ],
   "source": [
    "#Research question 1:\n",
    "data1 = data.drop(['timestamp', 'user_id'], axis=1)\n",
    "data1.to_json('RQ1_dataset.json.gz', orient='records', lines=True, compression='gzip')\n",
    "print('finished writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing\n"
     ]
    }
   ],
   "source": [
    "#Research question 2:\n",
    "data2 = data.drop(['book_id','rating','timestamp'],axis=1)\n",
    "data2.to_json('RQ2_dataset.json.gz', orient='records', lines=True, compression='gzip')\n",
    "print('finished writing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing\n"
     ]
    }
   ],
   "source": [
    "#Research question 3:\n",
    "data3 = data.drop(['book_id', 'rating', 'user_id'], axis=1)\n",
    "data3.to_json('RQ3_dataset.json.gz',orient='records',lines=True, compression='gzip')\n",
    "print('finished writing')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
